# 12. Naive Bayes Classification

> Use WEKA or Python to apply Naive Bayes on a spam/ham email dataset. Evaluate the model using confusion matrix.

---

## Naïve Bayes in WEKA

### Steps:

#### 1. Load Dataset

- Open WEKA → Explorer → **Preprocess** tab.
- Load a spam dataset (e.g., `spam.arff` from UCI SpamBase or email word frequency dataset).
- Example attributes: word frequencies, capital letters count, etc.

#### 2. Apply Naïve Bayes

- Go to **Classify** tab → Select classifier:
  **`weka.classifiers.bayes.NaiveBayes`**
- Choose **Cross-validation** (10-fold) or **Percentage split** (e.g., 70:30).

#### 3. Evaluate

WEKA will output:

- Confusion Matrix
- Accuracy, Precision, Recall, F1-score

### Example Confusion Matrix (spam dataset):

```
  === Confusion Matrix ===

    a   b   <-- classified as
   450  20 | a = ham
    30 500 | b = spam
```

**Interpretation:**

- 450 = True Ham
- 500 = True Spam
- 20 = Ham misclassified as Spam (False Positive)
- 30 = Spam misclassified as Ham (False Negative)

---

## Naïve Bayes in Python (scikit-learn)

### Load Dataset

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Sample spam/ham dataset
data = {
    'message': [
        'Free money now', 'Hi how are you', 'Win a prize', 
        'Meeting at 3pm', 'Claim your reward', 'Lunch tomorrow?',
        'Discount offer', 'See you later'
    ],
    'label': [1, 0, 1, 0, 1, 0, 1, 0]  # 1 = spam, 0 = ham
}

df = pd.DataFrame(data)
```

### Preprocess Text Data

```python
# Convert text to numerical features using Bag of Words
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(df['message']).toarray()
y = df['label']
```

### Split Dataset

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)
```

### Train Naive Bayes Model

```python
# Use MultinomialNB for text data
model = MultinomialNB()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)
```

### Evaluate Model

```python
# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))
```

### Confusion Matrix

```python
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

# Visualize confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Naive Bayes')
plt.show()
```

---

## Using a Real Spam Dataset (SMS Spam Collection)

### Load SMS Spam Dataset

```python
# Download from: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection
df = pd.read_csv('spam.csv', encoding='latin-1')
df = df[['v1', 'v2']]  # Keep only label and message columns
df.columns = ['label', 'message']

# Convert labels to binary
df['label'] = df['label'].map({'ham': 0, 'spam': 1})
```

### Vectorize and Train

```python
# Bag of words
vectorizer = CountVectorizer(max_features=500)
X = vectorizer.fit_transform(df['message']).toarray()
y = df['label']

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train
model = MultinomialNB()
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%")
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))
```

---

## Summary

| **Tool** | **Dataset Type** | **Best For** |
|----------|------------------|--------------|
| **WEKA** | Numeric/ARFF | Quick prototyping |
| **Python (sklearn)** | Text/Numeric | Production, large datasets |

### Naive Bayes Variants:
- **GaussianNB**: For continuous numeric features
- **MultinomialNB**: For discrete features (e.g., word counts in text)
- **BernoulliNB**: For binary features
