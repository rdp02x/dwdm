# 10. Decision Tree Classification10. Decision Tree Classification

Build a decision tree using ID3 or J48 algorithm 

> Build a decision tree using ID3 or J48 algorithm on a sample dataset (e.g., weather or bank loan data). Visualize the tree and analyze results.on a sample dataset (e.g., weather or bank loan 

data). Visualize the tree and analyze results.

---Decision Tree in WEKA (J48)

Example Dataset: Weather.arff

## Decision Tree in WEKA (J48)

@relation weather

### Example Dataset: Weather.arff

@attribute outlook {sunny, overcast, rainy}

```

@relation weather@attribute temperature {hot, mild, cool}



@attribute outlook {sunny, overcast, rainy}@attribute humidity {high, normal}

@attribute temperature {hot, mild, cool}

@attribute humidity {high, normal}@attribute windy {TRUE, FALSE}

@attribute windy {TRUE, FALSE}

@attribute play {yes, no}@attribute play {yes, no}



@data@data

sunny,hot,high,FALSE,no

sunny,hot,high,TRUE,nosunny,hot,high,FALSE,no

overcast,hot,high,FALSE,yes

rainy,mild,high,FALSE,yessunny,hot,high,TRUE,no

rainy,cool,normal,FALSE,yes

rainy,cool,normal,TRUE,noovercast,hot,high,FALSE,yes

overcast,cool,normal,TRUE,yes

sunny,mild,high,FALSE,norainy,mild,high,FALSE,yes

sunny,cool,normal,FALSE,yes

rainy,mild,normal,FALSE,yesrainy,cool,normal,FALSE,yes

sunny,mild,normal,TRUE,yes

overcast,mild,high,TRUE,yesrainy,cool,normal,TRUE,no

overcast,hot,normal,FALSE,yes

rainy,mild,high,TRUE,noovercast,cool,normal,TRUE,yes

```

sunny,mild,high,FALSE,no

### Steps in WEKA:



1. Open **Explorer → Preprocess** tab. Load `weather.arff`.

2. Go to **Classify** tab → Choose **J48** (`weka.classifiers.trees.J48`).sunny,cool,normal,FALSE,yes

3. Set parameters: `-C 0.25` (confidence factor), `-M 2` (min num instances per leaf).

4. Select **Cross-validation** (10-fold) or **Percentage split** (e.g., 70%).rainy,mild,normal,FALSE,yes

5. Click **Start**.

6. **Visualize tree**: Right-click on result → **Visualize tree**.sunny,mild,normal,TRUE,yes



### Example Output:overcast,mild,high,TRUE,yes



```overcast,hot,normal,FALSE,yes

=== Summary ===

Correctly Classified Instances          12               85.71 %rainy,mild,high,TRUE,no

Incorrectly Classified Instances         2               14.29 %

Steps in WEKA:

=== Confusion Matrix ===

 a  b   <-- classified asOpen Explorer → Preprocess tab. Load weather.arff.

 8  1 |  a = yes

 1  4 |  b = noGo to Classify tab → Choose J48 (weka.classifiers.trees.J48).

```

Set parameters: -C 0.25 (confidence factor), -M 2 (min num instances per leaf).

---


## Decision Tree in Python (scikit-learn)

### Load and Prepare Data

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn import tree
import matplotlib.pyplot as plt

# Sample weather dataset
data = {
    'outlook': [0, 0, 1, 2, 2, 2, 1, 0, 0, 2, 0, 1, 1, 2],
    'temperature': [0, 0, 0, 1, 2, 2, 2, 1, 2, 1, 1, 1, 0, 1],
    'humidity': [0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0],
    'windy': [0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1],
    'play': [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]
}

df = pd.DataFrame(data)
X = df.drop('play', axis=1)
y = df['play']
```

### Train Decision Tree

```python
# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Train model
clf = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)
clf.fit(X_train, y_train)

# Predictions
y_pred = clf.predict(X_test)
```

### Evaluate Model

```python
# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['No', 'Yes']))
```

### Visualize Decision Tree

```python
plt.figure(figsize=(12, 8))
tree.plot_tree(
    clf, 
    feature_names=['outlook', 'temperature', 'humidity', 'windy'],
    class_names=['No', 'Yes'],
    filled=True
)
plt.title("Decision Tree for Weather Dataset")
plt.show()
```

---

## Key Concepts

| **Concept** | **Description** |
|-------------|-----------------|
| **ID3** | Uses Information Gain (entropy-based) |
| **J48** | WEKA implementation of C4.5 (handles continuous & missing values) |
| **Pruning** | Reduces overfitting by removing branches |
| **Confusion Matrix** | Shows True/False Positives and Negatives |

---

## Summary

- **WEKA**: Quick GUI-based approach with J48
- **Python**: More control, visualization with `sklearn` and `matplotlib`
