# 10. Decision Tree Classification

> Build a decision tree using ID3 or J48 algorithm on a sample dataset (e.g., weather or bank loan data). Visualize the tree and analyze results.

---

## Decision Tree in WEKA (J48)

### Example Dataset: Weather.arff

```
@relation weather

@attribute outlook {sunny, overcast, rainy}
@attribute temperature {hot, mild, cool}
@attribute humidity {high, normal}
@attribute windy {TRUE, FALSE}
@attribute play {yes, no}

@data
sunny,hot,high,FALSE,no
sunny,hot,high,TRUE,no
overcast,hot,high,FALSE,yes
rainy,mild,high,FALSE,yes
rainy,cool,normal,FALSE,yes
rainy,cool,normal,TRUE,no
overcast,cool,normal,TRUE,yes
sunny,mild,high,FALSE,no
sunny,cool,normal,FALSE,yes
rainy,mild,normal,FALSE,yes
sunny,mild,normal,TRUE,yes
overcast,mild,high,TRUE,yes
overcast,hot,normal,FALSE,yes
rainy,mild,high,TRUE,no
```

### Steps in WEKA:

1. Open **Explorer → Preprocess** tab. Load `weather.arff`.
2. Go to **Classify** tab → Choose **J48** (`weka.classifiers.trees.J48`).
3. Set parameters: `-C 0.25` (confidence factor), `-M 2` (min num instances per leaf).
4. Select **Cross-validation** (10-fold) or **Percentage split** (e.g., 70%).
5. Click **Start**.
6. **Visualize tree**: Right-click on result → **Visualize tree**.

### Example Output:

```
=== Summary ===
Correctly Classified Instances          12               85.71 %
Incorrectly Classified Instances         2               14.29 %

=== Confusion Matrix ===
 a  b   <-- classified as
 8  1 |  a = yes
 1  4 |  b = no
```

---


## Decision Tree in Python (scikit-learn)

### Load and Prepare Data

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn import tree
import matplotlib.pyplot as plt

# Sample weather dataset
data = {
    'outlook': [0, 0, 1, 2, 2, 2, 1, 0, 0, 2, 0, 1, 1, 2],
    'temperature': [0, 0, 0, 1, 2, 2, 2, 1, 2, 1, 1, 1, 0, 1],
    'humidity': [0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0],
    'windy': [0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1],
    'play': [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]
}

df = pd.DataFrame(data)
X = df.drop('play', axis=1)
y = df['play']
```

### Train Decision Tree

```python
# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Train model
clf = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)
clf.fit(X_train, y_train)

# Predictions
y_pred = clf.predict(X_test)
```

### Evaluate Model

```python
# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['No', 'Yes']))
```

### Visualize Decision Tree

```python
plt.figure(figsize=(12, 8))
tree.plot_tree(
    clf, 
    feature_names=['outlook', 'temperature', 'humidity', 'windy'],
    class_names=['No', 'Yes'],
    filled=True
)
plt.title("Decision Tree for Weather Dataset")
plt.show()
```

---

## Key Concepts

| **Concept** | **Description** |
|-------------|-----------------|
| **ID3** | Uses Information Gain (entropy-based) |
| **J48** | WEKA implementation of C4.5 (handles continuous & missing values) |
| **Pruning** | Reduces overfitting by removing branches |
| **Confusion Matrix** | Shows True/False Positives and Negatives |

---

## Summary

- **WEKA**: Quick GUI-based approach with J48
- **Python**: More control, visualization with `sklearn` and `matplotlib`
