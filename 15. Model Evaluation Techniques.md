# 15. Model Evaluation Techniques

> Compare different classification models (Decision Tree, SVM, KNN) using precision, recall, F1-score, and ROC curves on a medical dataset.

---

## Import Libraries

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, roc_curve, auc, RocCurveDisplay
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
```

---

## Step 1: Load Dataset

```python
# Load breast cancer dataset
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)

print("Dataset shape:", X.shape)
print("Classes:", data.target_names)
```

---

## Step 2: Train-Test Split

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Scale features (important for SVM & KNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

---

## Step 3: Define Models

```python
models = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "SVM": SVC(probability=True, kernel="rbf", random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5)
}
```

---

## Step 4: Train, Evaluate, Compare

### Classification Reports

```python
results = {}

for name, model in models.items():
    # Train
    model.fit(X_train_scaled, y_train)
    
    # Predictions
    y_pred = model.predict(X_test_scaled)
    
    # Classification report (precision, recall, f1)
    print(f"\n{'='*60}")
    print(f"{name} Classification Report:")
    print(f"{'='*60}")
    print(classification_report(y_test, y_pred, target_names=data.target_names))
    
    # Store results
    results[name] = y_pred
```

---

## Step 5: ROC Curve Comparison

```python
plt.figure(figsize=(10, 7))

for name, model in models.items():
    # Get probability predictions
    y_proba = model.predict_proba(X_test_scaled)[:, 1]
    
    # Calculate ROC curve
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)
    
    # Plot
    plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})", linewidth=2)

# Diagonal line (random classifier)
plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')

plt.xlabel("False Positive Rate", fontsize=12)
plt.ylabel("True Positive Rate", fontsize=12)
plt.title("ROC Curve Comparison", fontsize=14, fontweight='bold')
plt.legend(loc="lower right", fontsize=10)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

---

## Step 6: Confusion Matrix for Each Model

```python
from sklearn.metrics import confusion_matrix

fig, axes = plt.subplots(1, 3, figsize=(15, 4))

for idx, (name, y_pred) in enumerate(results.items()):
    cm = confusion_matrix(y_test, y_pred)
    
    sns.heatmap(
        cm, 
        annot=True, 
        fmt='d', 
        cmap='Blues', 
        ax=axes[idx],
        xticklabels=data.target_names,
        yticklabels=data.target_names
    )
    axes[idx].set_title(f'{name} Confusion Matrix')
    axes[idx].set_xlabel('Predicted')
    axes[idx].set_ylabel('Actual')

plt.tight_layout()
plt.show()
```

---

## Step 7: Model Comparison Summary

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Create comparison dataframe
comparison = []

for name, model in models.items():
    y_pred = model.predict(X_test_scaled)
    
    comparison.append({
        'Model': name,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred),
        'Recall': recall_score(y_test, y_pred),
        'F1-Score': f1_score(y_test, y_pred)
    })

comparison_df = pd.DataFrame(comparison)
print("\n" + "="*60)
print("Model Comparison Summary:")
print("="*60)
print(comparison_df.to_string(index=False))
```

### Visualize Comparison

```python
# Melt dataframe for plotting
comparison_melted = comparison_df.melt(id_vars='Model', var_name='Metric', value_name='Score')

plt.figure(figsize=(12, 6))
sns.barplot(data=comparison_melted, x='Metric', y='Score', hue='Model', palette='Set2')
plt.title("Model Performance Comparison", fontsize=14, fontweight='bold')
plt.ylabel("Score", fontsize=12)
plt.xlabel("Evaluation Metric", fontsize=12)
plt.ylim(0, 1)
plt.legend(title="Model", fontsize=10)
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()
```

---

## Key Metrics Explained

| **Metric** | **Description** | **Formula** |
|------------|-----------------|-------------|
| **Accuracy** | Overall correctness | (TP + TN) / Total |
| **Precision** | Correctness of positive predictions | TP / (TP + FP) |
| **Recall** | Coverage of actual positives | TP / (TP + FN) |
| **F1-Score** | Harmonic mean of precision & recall | 2 × (Precision × Recall) / (Precision + Recall) |
| **ROC-AUC** | Area under ROC curve | Higher = better discrimination |

Where:
- **TP** = True Positives
- **TN** = True Negatives
- **FP** = False Positives
- **FN** = False Negatives

---

## Summary

**Best practices:**
- Use **Accuracy** for balanced datasets
- Use **Precision** when false positives are costly
- Use **Recall** when false negatives are costly
- Use **F1-Score** for imbalanced datasets
- Use **ROC-AUC** to compare models overall
